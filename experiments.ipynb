{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'bayesopt_playground' already exists and is not an empty directory.\n",
      "fatal: destination path 'HPOBench' already exists and is not an empty directory.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'child' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/IPython/utils/_process_posix.py:148\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     child \u001b[39m=\u001b[39m pexpect\u001b[39m.\u001b[39;49mspawn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msh, args\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m-c\u001b[39;49m\u001b[39m'\u001b[39;49m, cmd])  \u001b[39m# Vanilla Pexpect\u001b[39;00m\n\u001b[1;32m    149\u001b[0m flush \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/pexpect/pty_spawn.py:205\u001b[0m, in \u001b[0;36mspawn.__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spawn(command, args, preexec_fn, dimensions)\n\u001b[1;32m    206\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_poll \u001b[39m=\u001b[39m use_poll\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/pexpect/pty_spawn.py:303\u001b[0m, in \u001b[0;36mspawn._spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m [a \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, \u001b[39mbytes\u001b[39m) \u001b[39melse\u001b[39;00m a\u001b[39m.\u001b[39mencode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding)\n\u001b[1;32m    301\u001b[0m                  \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs]\n\u001b[0;32m--> 303\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mptyproc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spawnpty(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[1;32m    304\u001b[0m                              cwd\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcwd, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    306\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mptyproc\u001b[39m.\u001b[39mpid\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/pexpect/pty_spawn.py:315\u001b[0m, in \u001b[0;36mspawn._spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m ptyprocess\u001b[39m.\u001b[39;49mPtyProcess\u001b[39m.\u001b[39;49mspawn(args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/ptyprocess/ptyprocess.py:315\u001b[0m, in \u001b[0;36mPtyProcess.spawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[0m\n\u001b[1;32m    314\u001b[0m os\u001b[39m.\u001b[39mclose(exec_err_pipe_write)\n\u001b[0;32m--> 315\u001b[0m exec_err_data \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mread(exec_err_pipe_read, \u001b[39m4096\u001b[39;49m)\n\u001b[1;32m    316\u001b[0m os\u001b[39m.\u001b[39mclose(exec_err_pipe_read)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mgit clone https://github.com/theo-long/bayesopt_playground.git\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mgit clone https://github.com/automl/HPOBench.git\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39;49msystem(\u001b[39m'\u001b[39;49m\u001b[39mcd HPOBench && pip install .[paramnet]\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install botorch\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/ipykernel/zmqshell.py:633\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_ns[\u001b[39m\"\u001b[39m\u001b[39m_exit_code\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m system(cmd)\n\u001b[1;32m    632\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_ns[\u001b[39m\"\u001b[39m\u001b[39m_exit_code\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m system(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvar_expand(cmd, depth\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/IPython/utils/_process_posix.py:164\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    159\u001b[0m         out_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(child\u001b[39m.\u001b[39mbefore)\n\u001b[1;32m    160\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[39m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[39m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     child\u001b[39m.\u001b[39msendline(\u001b[39mchr\u001b[39m(\u001b[39m3\u001b[39m))\n\u001b[1;32m    165\u001b[0m     \u001b[39m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# way out.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'child' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# For colab\n",
    "# !git clone https://github.com/theo-long/bayesopt_playground.git\n",
    "# !git clone https://github.com/automl/HPOBench.git\n",
    "# !cd HPOBench && pip install .[paramnet]\n",
    "# !pip install botorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from bayesopt_playground.optimization_loops import optimization_loop, get_recommendation\n",
    "from bayesopt_playground.acquistion_functions import (\n",
    "    mutli_fidelity_entropy_search,\n",
    "    expected_improvement,\n",
    "    multi_fidelity_kg,\n",
    "    max_value_entropy_search,\n",
    "    knowledge_gradient,\n",
    ")\n",
    "from bayesopt_playground.models import multi_fidelity_gp, simple_gp\n",
    "from bayesopt_playground.benchmarks import (\n",
    "    generate_benchmark_bounds,\n",
    "    generate_objective_function,\n",
    "    svm_benchmark,\n",
    "    nn_benchmark,\n",
    ")\n",
    "from bayesopt_playground.utils import run_experiment\n",
    "\n",
    "from bayesopt_playground.benchmarks import (\n",
    "    augmented_branin_benchmark,\n",
    "    hartmann_benchmark,\n",
    "    branin_benchmark,\n",
    "    augmented_hartmann_benchmark,\n",
    "    nn_mnist_benchmark,\n",
    "    nn_fashion_mnist_benchmark,\n",
    "    generate_optimization_task,\n",
    "    svm_benchmark,\n",
    "    paramnet_opt_digits,\n",
    "    paramnet_adult,\n",
    "    paramnet_letter,\n",
    ")\n",
    "\n",
    "from botorch.models.cost import AffineFidelityCostModel\n",
    "from gpytorch.kernels import MaternKernel, RBFKernel\n",
    "from botorch.optim.fit import fit_gpytorch_mll_scipy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Expected Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Value Entropy Search (no Fidelity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting branin_benchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 7\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:35<00:00,  4.79s/it]\n",
      "100%|██████████| 20/20 [01:36<00:00,  4.83s/it]\n",
      "100%|██████████| 20/20 [01:38<00:00,  4.92s/it]\n",
      "100%|██████████| 20/20 [01:37<00:00,  4.89s/it]\n",
      "100%|██████████| 20/20 [01:38<00:00,  4.90s/it]\n",
      "100%|██████████| 20/20 [01:38<00:00,  4.95s/it]\n",
      "100%|██████████| 20/20 [01:40<00:00,  5.03s/it]\n",
      " 95%|█████████▌| 19/20 [01:48<00:04,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hartmann_benchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:51<00:00,  5.58s/it]\n",
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "/Users/tlong/anaconda3/envs/bayesopt/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:10<00:00,  6.50s/it]\n",
      "100%|██████████| 20/20 [02:10<00:00,  6.55s/it]\n",
      "100%|██████████| 20/20 [02:10<00:00,  6.55s/it]\n",
      "100%|██████████| 20/20 [02:12<00:00,  6.63s/it]\n",
      "100%|██████████| 20/20 [02:15<00:00,  6.76s/it]\n",
      "100%|██████████| 20/20 [02:15<00:00,  6.79s/it]\n",
      "100%|██████████| 20/20 [02:18<00:00,  6.93s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.12s/it]\n"
     ]
    }
   ],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'no_fidelity_max_value_entropy',\n",
    "    [\n",
    "        branin_benchmark, \n",
    "        hartmann_benchmark, \n",
    "        ], \n",
    "    simple_gp, \n",
    "    cost_model, \n",
    "    max_value_entropy_search,\n",
    "    multi_fidelity=False,\n",
    "    num_runs=8,\n",
    "    num_iters=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Gradient (no fidelity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'no_fidelity_knowledge_gradient',\n",
    "    [\n",
    "        branin_benchmark, \n",
    "        hartmann_benchmark], \n",
    "    simple_gp, \n",
    "    cost_model, \n",
    "    knowledge_gradient,\n",
    "    multi_fidelity=False,\n",
    "    num_runs=8,\n",
    "    num_iters=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVES with Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_branin_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:52<00:00,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_hartmann_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:04<00:00,  9.21s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_factory = lambda *args, **kwargs : multi_fidelity_gp(*args, kernel=RBFKernel)\n",
    "\n",
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_max_value_entropy',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    cost_model, \n",
    "    mutli_fidelity_entropy_search,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=1,\n",
    "    num_iters=20,\n",
    "    parallelized=False,\n",
    "    optimizer=fit_gpytorch_mll_scipy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_branin_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:12<00:00,  6.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_hartmann_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:00<00:00,  6.05s/it]\n"
     ]
    }
   ],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_max_value_entropy_2',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    cost_model, \n",
    "    mutli_fidelity_entropy_search,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=1,\n",
    "    num_iters=20,\n",
    "    parallelized=False,\n",
    "    optimizer=fit_gpytorch_mll_scipy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_branin_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:59<00:00,  5.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_hartmann_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:06<00:00,  9.33s/it]\n"
     ]
    }
   ],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_max_value_entropy_3',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    cost_model, \n",
    "    mutli_fidelity_entropy_search,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=1,\n",
    "    num_iters=20,\n",
    "    parallelized=False,\n",
    "    optimizer=fit_gpytorch_mll_scipy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_branin_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:00<00:00,  6.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_hartmann_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:17<00:00,  9.89s/it]\n"
     ]
    }
   ],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_max_value_entropy_4',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    cost_model, \n",
    "    mutli_fidelity_entropy_search,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=1,\n",
    "    num_iters=20,\n",
    "    parallelized=False,\n",
    "    optimizer=fit_gpytorch_mll_scipy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_branin_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:48<00:00,  5.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_hartmann_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:54<00:00,  8.71s/it]\n"
     ]
    }
   ],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_max_value_entropy_5',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    cost_model, \n",
    "    mutli_fidelity_entropy_search,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=1,\n",
    "    num_iters=20,\n",
    "    parallelized=False,\n",
    "    optimizer=fit_gpytorch_mll_scipy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_branin_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:45<00:00,  5.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_hartmann_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:23<00:00,  7.15s/it]\n"
     ]
    }
   ],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_max_value_entropy_6',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    cost_model, \n",
    "    mutli_fidelity_entropy_search,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=1,\n",
    "    num_iters=20,\n",
    "    parallelized=False,\n",
    "    optimizer=fit_gpytorch_mll_scipy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_branin_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:50<00:00,  5.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_hartmann_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:50<00:00,  8.53s/it]\n"
     ]
    }
   ],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_max_value_entropy_7',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    cost_model, \n",
    "    mutli_fidelity_entropy_search,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=1,\n",
    "    num_iters=20,\n",
    "    parallelized=False,\n",
    "    optimizer=fit_gpytorch_mll_scipy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_branin_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:58<00:00,  5.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_hartmann_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:37<00:00,  7.88s/it]\n"
     ]
    }
   ],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_max_value_entropy_8',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    cost_model, \n",
    "    mutli_fidelity_entropy_search,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=1,\n",
    "    num_iters=20,\n",
    "    parallelized=False,\n",
    "    optimizer=fit_gpytorch_mll_scipy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Grad with Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_branin_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMDNotImplementedError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/botorch/utils/dispatcher.py:93\u001b[0m, in \u001b[0;36mDispatcher.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     94\u001b[0m \u001b[39mexcept\u001b[39;00m MDNotImplementedError:\n\u001b[1;32m     95\u001b[0m     \u001b[39m# Traverses registered methods in order, yields whenever a match is found\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/botorch/fit.py:337\u001b[0m, in \u001b[0;36m_fit_multioutput_independent\u001b[0;34m(mll, _, __, closure, sequential, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39mif\u001b[39;00m (  \u001b[39m# incompatible models\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     \u001b[39mnot\u001b[39;00m sequential\n\u001b[1;32m    333\u001b[0m     \u001b[39mor\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    334\u001b[0m     \u001b[39mor\u001b[39;00m mll\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnum_outputs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    335\u001b[0m     \u001b[39mor\u001b[39;00m mll\u001b[39m.\u001b[39mlikelihood \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(mll\u001b[39m.\u001b[39mmodel, \u001b[39m\"\u001b[39m\u001b[39mlikelihood\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    336\u001b[0m ):\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mraise\u001b[39;00m MDNotImplementedError  \u001b[39m# defer to generic\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[39m# TODO: Unpacking of OutcomeTransforms not yet supported. Targets are often\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39m# pre-transformed in __init__, so try fitting with outcome_transform hidden\u001b[39;00m\n",
      "\u001b[0;31mMDNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cost_model \u001b[39m=\u001b[39m AffineFidelityCostModel()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m _ \u001b[39m=\u001b[39m run_experiment(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mmf_knowledge_gradient\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     [\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         augmented_branin_benchmark, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         augmented_hartmann_benchmark, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         ],  \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     multi_fidelity_gp,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     cost_model,  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     multi_fidelity_kg,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     multi_fidelity\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     num_runs\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     num_iters\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     parallelized\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tlong/Documents/code/bayesopt_playground/experiments.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/code/bayesopt_playground/utils.py:87\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(experiment_name, benchmarks, model_factory, cost_model_factory, acquisition_factory, multi_fidelity, num_runs, num_iters, initial_samples, parallelized, optimizer)\u001b[0m\n\u001b[1;32m     85\u001b[0m     iterable \u001b[39m=\u001b[39m args_iterable()\n\u001b[1;32m     86\u001b[0m     \u001b[39mfor\u001b[39;00m args \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m---> 87\u001b[0m         results\u001b[39m.\u001b[39mappend(single_run(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m     89\u001b[0m all_results \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m results\n\u001b[1;32m     91\u001b[0m \u001b[39m# Save after every benchmark in case of partial run\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/bayesopt_playground/utils.py:117\u001b[0m, in \u001b[0;36msingle_run\u001b[0;34m(run, model_factory, cost_model_factory, acquisition_factory, objective_function, bounds, num_iters, initial_samples, fidelity_samples, log_transform_indices, full_fidelity, pass_current_best, benchmark, optimizer)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msingle_run\u001b[39m(\n\u001b[1;32m    100\u001b[0m     run,\n\u001b[1;32m    101\u001b[0m     model_factory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m     optimizer,\n\u001b[1;32m    114\u001b[0m ):\n\u001b[1;32m    115\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRun \u001b[39m\u001b[39m{\u001b[39;00mrun\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m     model, train_x, train_obj, full_results \u001b[39m=\u001b[39m optimization_loop(\n\u001b[1;32m    118\u001b[0m         model_factory,\n\u001b[1;32m    119\u001b[0m         cost_model_factory,\n\u001b[1;32m    120\u001b[0m         acquisition_factory,\n\u001b[1;32m    121\u001b[0m         objective_function,\n\u001b[1;32m    122\u001b[0m         bounds,\n\u001b[1;32m    123\u001b[0m         n_iter\u001b[39m=\u001b[39;49mnum_iters,\n\u001b[1;32m    124\u001b[0m         initial_samples\u001b[39m=\u001b[39;49minitial_samples,\n\u001b[1;32m    125\u001b[0m         fidelity_samples\u001b[39m=\u001b[39;49mfidelity_samples,\n\u001b[1;32m    126\u001b[0m         log_transform_indices\u001b[39m=\u001b[39;49mlog_transform_indices,\n\u001b[1;32m    127\u001b[0m         full_fidelity\u001b[39m=\u001b[39;49mfull_fidelity,\n\u001b[1;32m    128\u001b[0m         pass_current_best\u001b[39m=\u001b[39;49mpass_current_best,\n\u001b[1;32m    129\u001b[0m         optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m     final_rec, objective_value, results \u001b[39m=\u001b[39m get_recommendation(\n\u001b[1;32m    132\u001b[0m         model, objective_function, bounds, full_fidelity\u001b[39m=\u001b[39mfull_fidelity\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    134\u001b[0m     full_results \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m results\n",
      "File \u001b[0;32m~/Documents/code/bayesopt_playground/optimization_loops.py:110\u001b[0m, in \u001b[0;36moptimization_loop\u001b[0;34m(model_factory, cost_model_factory, acquisition_factory, objective_function, bounds, initial_samples, fidelity_samples, n_iter, q, log_transform_indices, pass_current_best, full_fidelity, optimizer)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mwith\u001b[39;00m gpt_settings\u001b[39m.\u001b[39mcholesky_max_tries(\u001b[39m6\u001b[39m):\n\u001b[1;32m    107\u001b[0m     \u001b[39m# Initialize objective model\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     mll, model \u001b[39m=\u001b[39m model_factory(train_x, train_obj, bounds, log_transform_indices\u001b[39m=\u001b[39mlog_transform_indices)\n\u001b[0;32m--> 110\u001b[0m     optimize_hyperparameters(mll, optimizer\u001b[39m=\u001b[39;49moptimizer)\n\u001b[1;32m    112\u001b[0m     \u001b[39m# Initialize cost model\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(cost_model_factory, AffineFidelityCostModel):\n",
      "File \u001b[0;32m~/Documents/code/bayesopt_playground/optimization_loops.py:25\u001b[0m, in \u001b[0;36moptimize_hyperparameters\u001b[0;34m(mll, optimizer, raise_error)\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer \u001b[39m=\u001b[39m optimizer \u001b[39mif\u001b[39;00m optimizer \u001b[39melse\u001b[39;00m fit_gpytorch_mll_torch\n\u001b[1;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     fit_gpytorch_mll(mll, optimizer\u001b[39m=\u001b[39;49mfit_gpytorch_mll_torch)\n\u001b[1;32m     26\u001b[0m \u001b[39mexcept\u001b[39;00m ModelFittingError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m     \u001b[39mif\u001b[39;00m raise_error:\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/botorch/fit.py:97\u001b[0m, in \u001b[0;36mfit_gpytorch_mll\u001b[0;34m(mll, closure, optimizer, closure_kwargs, optimizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m optimizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# defer to per-method defaults\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m optimizer\n\u001b[0;32m---> 97\u001b[0m \u001b[39mreturn\u001b[39;00m FitGPyTorchMLL(\n\u001b[1;32m     98\u001b[0m     mll,\n\u001b[1;32m     99\u001b[0m     \u001b[39mtype\u001b[39;49m(mll\u001b[39m.\u001b[39;49mlikelihood),\n\u001b[1;32m    100\u001b[0m     \u001b[39mtype\u001b[39;49m(mll\u001b[39m.\u001b[39;49mmodel),\n\u001b[1;32m    101\u001b[0m     closure\u001b[39m=\u001b[39;49mclosure,\n\u001b[1;32m    102\u001b[0m     closure_kwargs\u001b[39m=\u001b[39;49mclosure_kwargs,\n\u001b[1;32m    103\u001b[0m     optimizer_kwargs\u001b[39m=\u001b[39;49moptimizer_kwargs,\n\u001b[1;32m    104\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    105\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/botorch/utils/dispatcher.py:100\u001b[0m, in \u001b[0;36mDispatcher.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mfor\u001b[39;00m func \u001b[39min\u001b[39;00m funcs:\n\u001b[1;32m     99\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    101\u001b[0m     \u001b[39mexcept\u001b[39;00m MDNotImplementedError:\n\u001b[1;32m    102\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/botorch/fit.py:244\u001b[0m, in \u001b[0;36m_fit_fallback\u001b[0;34m(mll, _, __, closure, optimizer, closure_kwargs, optimizer_kwargs, max_attempts, warning_handler, caught_exception_types, **ignore)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mwith\u001b[39;00m catch_warnings(record\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m warning_list, debug(\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    243\u001b[0m     simplefilter(\u001b[39m\"\u001b[39m\u001b[39malways\u001b[39m\u001b[39m\"\u001b[39m, category\u001b[39m=\u001b[39mOptimizationWarning)\n\u001b[0;32m--> 244\u001b[0m     optimizer(mll, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptimizer_kwargs)\n\u001b[1;32m    246\u001b[0m \u001b[39m# Resolved warnings and determine whether or not to retry\u001b[39;00m\n\u001b[1;32m    247\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/botorch/optim/fit.py:185\u001b[0m, in \u001b[0;36mfit_gpytorch_mll_torch\u001b[0;34m(mll, parameters, bounds, closure, closure_kwargs, step_limit, stopping_criterion, optimizer, scheduler, callback)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m closure_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     closure \u001b[39m=\u001b[39m partial(closure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mclosure_kwargs)\n\u001b[0;32m--> 185\u001b[0m \u001b[39mreturn\u001b[39;00m torch_minimize(\n\u001b[1;32m    186\u001b[0m     closure\u001b[39m=\u001b[39;49mclosure,\n\u001b[1;32m    187\u001b[0m     parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    188\u001b[0m     bounds\u001b[39m=\u001b[39;49mbounds_dict \u001b[39mif\u001b[39;49;00m bounds \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbounds_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbounds},\n\u001b[1;32m    189\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    190\u001b[0m     scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[1;32m    191\u001b[0m     step_limit\u001b[39m=\u001b[39;49mstep_limit,\n\u001b[1;32m    192\u001b[0m     stopping_criterion\u001b[39m=\u001b[39;49mstopping_criterion,\n\u001b[1;32m    193\u001b[0m     callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    194\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/botorch/optim/core.py:185\u001b[0m, in \u001b[0;36mtorch_minimize\u001b[0;34m(closure, parameters, bounds, callback, optimizer, scheduler, step_limit, stopping_criterion)\u001b[0m\n\u001b[1;32m    183\u001b[0m result: OptimizationResult\n\u001b[1;32m    184\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(step_limit):\n\u001b[0;32m--> 185\u001b[0m     fval, _ \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    186\u001b[0m     result \u001b[39m=\u001b[39m OptimizationResult(\n\u001b[1;32m    187\u001b[0m         step\u001b[39m=\u001b[39mstep,\n\u001b[1;32m    188\u001b[0m         fval\u001b[39m=\u001b[39mfval\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mitem(),\n\u001b[1;32m    189\u001b[0m         status\u001b[39m=\u001b[39mOptimizationStatus\u001b[39m.\u001b[39mRUNNING,\n\u001b[1;32m    190\u001b[0m         runtime\u001b[39m=\u001b[39mmonotonic() \u001b[39m-\u001b[39m start_time,\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    193\u001b[0m     \u001b[39m# TODO: Update stopping_criterion API to return a message.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/botorch/optim/closures/core.py:64\u001b[0m, in \u001b[0;36mForwardBackwardClosure.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Tuple[Optional[Tensor], \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]:\n\u001b[1;32m     63\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_manager():\n\u001b[0;32m---> 64\u001b[0m         values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m         value \u001b[39m=\u001b[39m values \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreducer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreducer(values)\n\u001b[1;32m     66\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward(value)\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/botorch/optim/closures/model_closures.py:175\u001b[0m, in \u001b[0;36m_get_loss_closure_exact_internal.<locals>.closure\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 175\u001b[0m     model_output \u001b[39m=\u001b[39m mll\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49mmll\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtrain_inputs)\n\u001b[1;32m    176\u001b[0m     log_likelihood \u001b[39m=\u001b[39m mll(\n\u001b[1;32m    177\u001b[0m         model_output, mll\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain_targets, \u001b[39m*\u001b[39mmll\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    178\u001b[0m     )\n\u001b[1;32m    179\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mlog_likelihood\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:258\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(torch\u001b[39m.\u001b[39mequal(train_input, \u001b[39minput\u001b[39m) \u001b[39mfor\u001b[39;00m train_input, \u001b[39minput\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(train_inputs, inputs)):\n\u001b[1;32m    257\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou must train on the training inputs!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 258\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    259\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[1;32m    261\u001b[0m \u001b[39m# Prior mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/gpytorch/module.py:30\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 30\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     32\u001b[0m         \u001b[39mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/botorch/models/gp_regression.py:179\u001b[0m, in \u001b[0;36mSingleTaskGP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    177\u001b[0m mean_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_module(x)\n\u001b[1;32m    178\u001b[0m covar_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcovar_module(x)\n\u001b[0;32m--> 179\u001b[0m \u001b[39mreturn\u001b[39;00m MultivariateNormal(mean_x, covar_x)\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/pyro/distributions/distribution.py:24\u001b[0m, in \u001b[0;36mDistributionMeta.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m---> 24\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/gpytorch/distributions/multivariate_normal.py:44\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[0;34m(self, mean, covariance_matrix, validate_args)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__unbroadcasted_scale_tril \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_args \u001b[39m=\u001b[39m validate_args\n\u001b[0;32m---> 44\u001b[0m batch_shape \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_shapes(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], covariance_matrix\u001b[39m.\u001b[39;49mshape[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m])\n\u001b[1;32m     46\u001b[0m event_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\n\u001b[1;32m     48\u001b[0m \u001b[39m# TODO: Integrate argument validation for LinearOperators into torch.distribution validation logic\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/linear_operator/operators/_linear_operator.py:2160\u001b[0m, in \u001b[0;36mLinearOperator.shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2158\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   2159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshape\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m-> 2160\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize()\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/linear_operator/operators/_linear_operator.py:2153\u001b[0m, in \u001b[0;36mLinearOperator.size\u001b[0;34m(self, dim)\u001b[0m\n\u001b[1;32m   2147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msize\u001b[39m(\u001b[39mself\u001b[39m, dim: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[torch\u001b[39m.\u001b[39mSize, \u001b[39mint\u001b[39m]:\n\u001b[1;32m   2148\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2149\u001b[0m \u001b[39m    Returns he size of the LinearOperator (or the specified dimension).\u001b[39;00m\n\u001b[1;32m   2150\u001b[0m \n\u001b[1;32m   2151\u001b[0m \u001b[39m    :param dim: A specific dimension.\u001b[39;00m\n\u001b[1;32m   2152\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2153\u001b[0m     size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_size()\n\u001b[1;32m   2154\u001b[0m     \u001b[39mif\u001b[39;00m dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2155\u001b[0m         \u001b[39mreturn\u001b[39;00m size[dim]\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_in_cache(\u001b[39mself\u001b[39m, cache_name, \u001b[39m*\u001b[39margs, kwargs_pkl\u001b[39m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m _add_to_cache(\u001b[39mself\u001b[39;49m, cache_name, method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs), \u001b[39m*\u001b[39;49margs, kwargs_pkl\u001b[39m=\u001b[39;49mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m _get_from_cache(\u001b[39mself\u001b[39m, cache_name, \u001b[39m*\u001b[39margs, kwargs_pkl\u001b[39m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/gpytorch/utils/memoize.py:84\u001b[0m, in \u001b[0;36m_add_to_cache\u001b[0;34m(obj, name, val, kwargs_pkl, *args)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_add_to_cache\u001b[39m(obj, name, val, \u001b[39m*\u001b[39margs, kwargs_pkl):\n\u001b[1;32m     83\u001b[0m     \u001b[39m\"\"\"Add a result to the cache of an object (honoring calling args).\"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39;49m(obj, \u001b[39m\"\u001b[39;49m\u001b[39m_memoize_cache\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n\u001b[1;32m     85\u001b[0m         obj\u001b[39m.\u001b[39m_memoize_cache \u001b[39m=\u001b[39m {}\n\u001b[1;32m     86\u001b[0m     obj\u001b[39m.\u001b[39m_memoize_cache[(name, args, kwargs_pkl)] \u001b[39m=\u001b[39m val\n",
      "File \u001b[0;32m~/anaconda3/envs/bayesopt/lib/python3.9/site-packages/gpytorch/lazy/lazy_tensor.py:59\u001b[0m, in \u001b[0;36mdeprecated_lazy_tensor.<locals>.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meigvalsh(), \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlazy_tensor\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m name:\n\u001b[1;32m     61\u001b[0m         new_name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mlazy_tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlinear_op\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_knowledge_gradient',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    cost_model,  \n",
    "    multi_fidelity_kg,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=8,\n",
    "    num_iters=20,\n",
    "    parallelized=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_branin_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:37<00:00, 10.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented_hartmann_benchmark\n",
      "Run 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [03:14<02:45, 18.40s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_knowledge_gradient_2',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    multi_fidelity_gp,  \n",
    "    multi_fidelity_kg,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=1,\n",
    "    num_iters=20,\n",
    "    parallelized=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_knowledge_gradient_3',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    multi_fidelity_gp,  \n",
    "    multi_fidelity_kg,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=1,\n",
    "    num_iters=20,\n",
    "    parallelized=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_knowledge_gradient_4',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    multi_fidelity_gp,  \n",
    "    multi_fidelity_kg,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=1,\n",
    "    num_iters=20,\n",
    "    parallelized=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_knowledge_gradient_5',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    multi_fidelity_gp,  \n",
    "    multi_fidelity_kg,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=1,\n",
    "    num_iters=20,\n",
    "    parallelized=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_knowledge_gradient_6',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    multi_fidelity_gp,  \n",
    "    multi_fidelity_kg,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=1,\n",
    "    num_iters=20,\n",
    "    parallelized=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_knowledge_gradient_7',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    multi_fidelity_gp,  \n",
    "    multi_fidelity_kg,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=1,\n",
    "    num_iters=20,\n",
    "    parallelized=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_knowledge_gradient_8',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    multi_fidelity_gp,  \n",
    "    multi_fidelity_kg,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=1,\n",
    "    num_iters=20,\n",
    "    parallelized=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try basic cost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_model = AffineFidelityCostModel()\n",
    "_ = run_experiment(\n",
    "    'mf_max_value_entropy_basic_cost',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    cost_model, \n",
    "    mutli_fidelity_entropy_search,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=8,\n",
    "    num_iters=20,\n",
    "    parallelized=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_experiment(\n",
    "    'mf_knowledge_gradient_basic_cost',\n",
    "    [\n",
    "        augmented_branin_benchmark, \n",
    "        augmented_hartmann_benchmark, \n",
    "        ],  \n",
    "    multi_fidelity_gp,\n",
    "    multi_fidelity_gp,  \n",
    "    multi_fidelity_kg,\n",
    "    multi_fidelity=True,\n",
    "    num_runs=8,\n",
    "    num_iters=20,\n",
    "    parallelized=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "files = glob.glob('./data/mf_max_value_entropy*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for i, f in enumerate(files):\n",
    "    df = pd.read_csv(f)\n",
    "    df['run'] = i\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/mf_max_value_entropy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"benchmark\"] == \"max_v\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bayesopt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4854f60f564f03b76d206430892f8b02f14fc2bd1a9b722c1b4a0859ead56e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
